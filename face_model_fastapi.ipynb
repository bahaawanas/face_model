{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3595cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\newvision\\appdata\\local\\temp\\pip-req-build-lpa54qxv\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\anaconda\\lib\\site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in c:\\anaconda\\lib\\site-packages (from clip==1.0) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied: torch in c:\\anaconda\\lib\\site-packages (from clip==1.0) (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\anaconda\\lib\\site-packages (from clip==1.0) (0.17.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\anaconda\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\anaconda\\lib\\site-packages (from torch->clip==1.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\anaconda\\lib\\site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda\\lib\\site-packages (from torch->clip==1.0) (2023.10.0)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\anaconda\\lib\\site-packages (from torchvision->clip==1.0) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\anaconda\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\NewVision\\AppData\\Local\\Temp\\pip-req-build-lpa54qxv'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799575e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'facenet_pytorch' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/timesler/facenet-pytorch.git facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31576e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "from sentence_transformers import util\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import io\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d38f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FastAPI\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f2dc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 896, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=896, out_features=896, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=896, out_features=3584, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3584, out_features=896, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=640, out_features=640, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=640, out_features=2560, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 640)\n",
       "  (ln_final): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14247030",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0b71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageEncoder(img):\n",
    "    if img.dtype != 'uint8':\n",
    "        img = img.astype('uint8')  # Convert to 8-bit if necessary\n",
    "    img1 = Image.fromarray(img).convert('RGB')\n",
    "    img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "    img1 = model.encode_image(img1)\n",
    "    return img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a4a75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_comp_img(person, person_id):\n",
    "    \n",
    "    person_crop_path = person.split(\".\")[0] + \"_crop.jpg\"\n",
    "    person_id_crop_path = person_id.split(\".\")[0] + \"_crop.jpg\"\n",
    "    \n",
    "    # If required, create a face detection pipeline using MTCNN:\n",
    "    mtcnn = MTCNN(image_size=160, margin=0)\n",
    "\n",
    "    # Create an inception resnet (in eval mode):\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "\n",
    "    person_img = Image.open(person)\n",
    "    person_id_img = Image.open(person_id)\n",
    "\n",
    "     # Get cropped and prewhitened image tensor\n",
    "    person_crop = mtcnn(person_img,save_path = person_crop_path)\n",
    "    person_id_crop = mtcnn(person_id_img, save_path = person_id_crop_path)\n",
    "    \n",
    "    # Calculate embedding (unsqueeze to add batch dimension)\n",
    "    person_crop_embedding = resnet(person_crop.unsqueeze(0))\n",
    "    person_id_crop_embedding = resnet(person_id_crop.unsqueeze(0))\n",
    "    \n",
    "    # Or, if using for VGGFace2 classification\n",
    "    resnet.classify = True\n",
    "\n",
    "    person_crop_probs = resnet(person_crop.unsqueeze(0))\n",
    "    person_id_crop_probs= resnet(person_id_crop.unsqueeze(0))\n",
    "\n",
    "\n",
    "    person_compare = cv2.imread(person_crop_path, cv2.IMREAD_UNCHANGED)\n",
    "    person_id_compare = cv2.imread(person_id_crop_path, cv2.IMREAD_UNCHANGED)\n",
    "    img1 = imageEncoder(person_compare)\n",
    "    img2 = imageEncoder(person_id_compare)\n",
    "    cos_scores = util.pytorch_cos_sim(img1, img2)\n",
    "    score = round(float(cos_scores[0][0]) * 100, 2)\n",
    "    return score, person_compare, person_id_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78e8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a3d312d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/compare/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompare_images\u001b[39m(file1: UploadFile \u001b[38;5;241m=\u001b[39m File(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m), file2: UploadFile \u001b[38;5;241m=\u001b[39m File(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# Read uploaded files\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         image1 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[38;5;28;01mawait\u001b[39;00m file1\u001b[38;5;241m.\u001b[39mread()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "@app.post(\"/compare/\")\n",
    "async def compare_images(file1: UploadFile = File(...), file2: UploadFile = File(...)):\n",
    "    try:\n",
    "        # Read uploaded files\n",
    "        image1 = Image.open(io.BytesIO(await file1.read()))\n",
    "        image2 = Image.open(io.BytesIO(await file2.read()))\n",
    "\n",
    "        # Generate score\n",
    "        score = crop_comp_img(image1, image2)\n",
    "        result = \"Same Person\" if score >= 60 else \"Not the same Person\"\n",
    "        return JSONResponse(content={\"score\": score, \"result\": result})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in compare_images endpoint: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Internal Server Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c328989",
   "metadata": {},
   "outputs": [],
   "source": [
    "bahaa = \"C:\\\\Users\\\\NewVision\\\\Downloads\\\\image_model\\\\bahaa.jpg\"\n",
    "bahaa_id = \"C:\\\\Users\\\\NewVision\\\\Downloads\\\\image_model\\\\front_id.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ba8aa5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m crop_comp_img(bahaa,bahaa_id)\n",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m, in \u001b[0;36mcrop_comp_img\u001b[1;34m(person_img, person_id_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrop_comp_img\u001b[39m(person_img, person_id_img):\n\u001b[0;32m      3\u001b[0m         mtcnn \u001b[38;5;241m=\u001b[39m MTCNN(image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m160\u001b[39m, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m         person_crop \u001b[38;5;241m=\u001b[39m mtcnn(person_img)\n\u001b[0;32m      5\u001b[0m         person_id_crop \u001b[38;5;241m=\u001b[39m mtcnn(person_id_img)\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m person_crop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m person_id_crop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\All models\\facenet_pytorch\\models\\mtcnn.py:258\u001b[0m, in \u001b[0;36mMTCNN.forward\u001b[1;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03mdetection and extraction of faces, returning tensors representing detected faces rather\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mthan the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m>>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m batch_boxes, batch_probs, batch_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect(img, landmarks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Select faces\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_all:\n",
      "File \u001b[1;32m~\\All models\\facenet_pytorch\\models\\mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[38;5;241m=\u001b[39m detect_face(\n\u001b[0;32m    314\u001b[0m         img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_face_size,\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpnet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monet,\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthresholds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactor,\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    318\u001b[0m     )\n\u001b[0;32m    320\u001b[0m boxes, probs, points \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[1;32m~\\All models\\facenet_pytorch\\models\\utils\\detect_face.py:38\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(imgs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m     37\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m [imgs]\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(img\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m imgs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMTCNN batch processing only compatible with equal-dimension images.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m imgs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([np\u001b[38;5;241m.\u001b[39muint8(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs])\n",
      "File \u001b[1;32m~\\All models\\facenet_pytorch\\models\\utils\\detect_face.py:38\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(imgs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m     37\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m [imgs]\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(img\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m imgs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMTCNN batch processing only compatible with equal-dimension images.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m imgs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([np\u001b[38;5;241m.\u001b[39muint8(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "crop_comp_img(bahaa,bahaa_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
